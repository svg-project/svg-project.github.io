<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Sparse VideoGen 2</title>
  <link rel="icon" type="image/x-icon" href="../static/images/svglogo.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="../static/css/bulma.min.css">
  <link rel="stylesheet" href="../static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="../static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="../static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="../static/css/index.css">

  <!-- Picture Modal Box Style -->
  <style>
    .image-modal {
      display: none;
      position: fixed;
      z-index: 1000;
      left: 0;
      top: 0;
      width: 100%;
      height: 100%;
      background-color: rgba(0, 0, 0, 0.9);
      cursor: pointer;
    }

    .image-modal-content {
      position: absolute;
      top: 50%;
      left: 50%;
      transform: translate(-50%, -50%);
      max-width: 90%;
      max-height: 90%;
      object-fit: contain;
      border-radius: 8px;
      box-shadow: 0 4px 20px rgba(0, 0, 0, 0.5);
    }

    .image-modal-close {
      position: absolute;
      top: 20px;
      right: 35px;
      color: #ffffff;
      font-size: 40px;
      font-weight: bold;
      cursor: pointer;
      z-index: 1001;
      transition: color 0.3s;
    }

    .image-modal-close:hover {
      color: #cccccc;
    }

    .image-modal-nav {
      position: absolute;
      top: 50%;
      transform: translateY(-50%);
      color: #ffffff;
      font-size: 30px;
      font-weight: bold;
      cursor: pointer;
      padding: 20px;
      user-select: none;
      transition: color 0.3s;
      z-index: 1001;
    }

    .image-modal-nav:hover {
      color: #cccccc;
    }

    .image-modal-prev {
      left: 20px;
    }

    .image-modal-next {
      right: 20px;
    }

    .clickable-image {
      cursor: pointer;
      transition: opacity 0.3s;
    }

    .clickable-image:hover {
      opacity: 0.8;
    }

    .image-modal-caption {
      position: absolute;
      bottom: 20px;
      left: 50%;
      transform: translateX(-50%);
      color: #ffffff;
      text-align: center;
      font-size: 16px;
      background-color: rgba(0, 0, 0, 0.7);
      padding: 10px 20px;
      border-radius: 4px;
      max-width: 80%;
    }

    @media (max-width: 768px) {
      .image-modal-content {
        max-width: 95%;
        max-height: 85%;
      }

      .image-modal-close {
        top: 10px;
        right: 15px;
        font-size: 30px;
      }

      .image-modal-nav {
        font-size: 24px;
        padding: 15px;
      }

      .image-modal-prev {
        left: 10px;
      }

      .image-modal-next {
        right: 10px;
      }

      .image-modal-caption {
        font-size: 14px;
        bottom: 10px;
        padding: 8px 15px;
      }
    }

    /* Image Gallery Grid Styles */
    .image-gallery-grid {
      display: grid;
      grid-template-columns: repeat(16, 1fr);
      grid-template-rows: repeat(20, 1fr);
      gap: 10px;
      width: 100vw;
      height: 150vh;
      margin: 0 auto;
      padding: 80px;
      max-width: none;
    }

    .gallery-item {
      border-radius: 12px;
      overflow: hidden;
      transition: all 0.3s ease;
      cursor: pointer;
    }

    .gallery-item:hover {
      transform: translateY(-2px);
      box-shadow: 0 8px 25px rgba(0, 0, 0, 0.15);
    }

    .image-placeholder {
      width: 100%;
      height: 100%;
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      display: flex;
      align-items: center;
      justify-content: center;
      color: white;
      font-size: 14px;
      font-weight: 600;
      text-shadow: 0 1px 2px rgba(0, 0, 0, 0.3);
      text-align: center;
      padding: 5px;
    }

    /* Grid positioning - 16x20 layout */
    .item-1-1 {
      grid-column: 1 / 5;
      grid-row: 1 / 6;
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    }

    .item-1-2 {
      grid-column: 5 / 13;
      grid-row: 1 / 6;
      background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
    }

    .item-1-3 {
      grid-column: 13 / 17;
      grid-row: 1 / 6;
      background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);
    }

    .item-2-1 {
      grid-column: 1 / 4;
      grid-row: 6/ 12;
      background: linear-gradient(135deg, #43e97b 0%, #38f9d7 100%);
    }

    .item-2-2 {
      grid-column: 4 / 9;
      grid-row: 6 / 12;
      background: linear-gradient(135deg, #fa709a 0%, #fee140 100%);
    }

    .item-2-3 {
      grid-column: 9 / 14;
      grid-row: 6 / 12;
      background: linear-gradient(135deg, #43e97b 0%, #38f9d7 100%);
    }

    .item-2-4 {
      grid-column: 14 / 17;
      grid-row: 6 / 12;
      background: linear-gradient(135deg, #fa709a 0%, #fee140 100%);
    }

    .item-3-1 {
      grid-column: 1 / 5;
      grid-row: 12 / 16;
      background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%);
    }

    .item-3-2 {
      grid-column: 13 / 17;
      grid-row: 12 / 17;
      background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%);
    }

    .item-3-3 {
      grid-column: 5 / 8;
      grid-row: 12 / 15;
      background: linear-gradient(135deg, #a8c0ff 0%, #3f2b96 100%);
    }

    .item-3-4 {
      grid-column: 8 / 13;
      grid-row: 12 / 15;
      background: linear-gradient(135deg, #fbc2eb 0%, #a6c1ee 100%);
    }

    .item-3-5 {
      grid-column: 5 / 9;
      grid-row: 15 / 18;
      background: linear-gradient(135deg, #a8c0ff 0%, #3f2b96 100%);
    }

    .item-3-6 {
      grid-column: 9 / 13;
      grid-row: 15 / 18;
      background: linear-gradient(135deg, #fbc2eb 0%, #a6c1ee 100%);
    }

    .item-4-1 {
      grid-column: 1 / 5;
      grid-row: 16 / 21;
      background: linear-gradient(135deg, #fdbb2d 0%, #22c1c3 100%);
    }

    .item-4-2 {
      grid-column: 13 / 17;
      grid-row: 17 / 21;
      background: linear-gradient(135deg, #ee9ca7 0%, #ffdde1 100%);
    }

    .item-4-3 {
      grid-column: 5 / 10;
      grid-row: 18 / 21;
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      border: 2px dashed #3273dc;
    }

    .item-4-4 {
      grid-column: 10 / 13;
      grid-row: 18 / 21;
      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      border: 2px dashed #3273dc;
    }

    /* Responsive design for gallery */
    @media (max-width: 2048px) {
      .image-gallery-grid {
        grid-template-columns: repeat(16, 1fr);
        grid-template-rows: repeat(20, 1fr);
        width: 95vw;
        height: 70vh;
        gap: 3px;
      }

      .item-1-1 {
        grid-column: 1 / 5;
        grid-row: 1 / 6;
      }

      .item-1-2 {
        grid-column: 5 / 13;
        grid-row: 1 / 6;
      }

      .item-1-3 {
        grid-column: 13 / 17;
        grid-row: 1 / 6;
      }

      .item-2-1 {
        grid-column: 1 / 4;
        grid-row: 6 / 12;
      }

      .item-2-2 {
        grid-column: 4 / 9;
        grid-row: 6 / 12;
      }

      .item-2-3 {
        grid-column: 9 / 14;
        grid-row: 6 / 12;
      }

      .item-2-4 {
        grid-column: 14 / 17;
        grid-row: 6 / 12;
      }

      .item-3-1 {
        grid-column: 1 / 5;
        grid-row: 12 / 16;
      }

      .item-3-2 {
        grid-column: 13 / 17;
        grid-row: 12 / 16;
      }

      .item-3-3 {
        grid-column: 5 / 8;
        grid-row: 12 / 15;
      }

      .item-3-4 {
        grid-column: 8 / 13;
        grid-row: 12 / 15;
      }

      .item-3-5 {
        grid-column: 5 / 9;
        grid-row: 15 / 18;
      }

      .item-3-6 {
        grid-column: 9 / 13;
        grid-row: 15 / 18;
      }

      .item-4-1 {
        grid-column: 1 / 5;
        grid-row: 16 / 21;
      }

      .item-4-2 {
        grid-column: 13 / 17;
        grid-row: 17 / 21;
      }

      .item-4-3 {
        grid-column: 5 / 10;
        grid-row: 18 / 21;
      }

      .item-4-4 {
        grid-column: 10 / 13;
        grid-row: 18 / 21;
      }

      .image-placeholder {
        font-size: 8px;
      }
    }
  </style>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="../static/js/fontawesome.all.min.js"></script>
  <script src="../static/js/bulma-carousel.min.js"></script>
  <script src="../static/js/bulma-slider.min.js"></script>
  <script src="../static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">Sparse VideoGen2: Accelerate Video Generation with Sparse Attention
              via Semantic-Aware Permutation

            </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://andy-yang-1.github.io/" target="_blank">Shuo Yang</a><sup>*</sup>,
              </span>
              <span class="author-block">
                <a href="https://haochengxi.github.io/" target="_blank">Haocheng Xi</a><sup>*</sup>,
              </span>
              <span class="author-block">
                <a href="https://happierpig.github.io/" target="_blank">Yilong Zhao</a>,
              </span>
              <span class="author-block">
                <a href="https://lmxyy.me/" target="_blank">Muyang Li</a>,
              </span>
              <span class="author-block">
                <a href="blank" target="_blank">Jintao Zhang</a>,
              </span>
              <span class="author-block">
                <a href="https://han-cai.github.io/" target="_blank">Han Cai</a>,
              </span>
              <span class="author-block">
                <a href="https://yujunlin.com/" target="_blank">Yujun Lin</a>,
              </span>
              <span class="author-block">
                <a href="https://xiuyuli.com/" target="_blank">Xiuyu Li</a>,
              </span>
              <span class="author-block">
                <a href="https://www.chenfengx.com/" target="_blank">Chenfeng Xu</a>,
              </span>
              <span class="author-block">
                <a href="https://x.com/ziqipeng?lang=en" target="_blank">Kelly Peng</a>,
              </span>
              <span class="author-block">
                <a href="https://ml.cs.tsinghua.edu.cn/~jianfei/" target="_blank">Jianfei Chen</a>,
              </span>
              <span class="author-block">
                <a href="https://hanlab.mit.edu/songhan" target="_blank">Song Han</a>,
              </span>
              <span class="author-block">
                <a href="https://people.eecs.berkeley.edu/~keutzer/" target="_blank">Kurt Keutzer</a>,
              </span>
              <span class="author-block">
                <a href="https://people.eecs.berkeley.edu/~istoica/" target="_blank">Ion Stoica</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup> University of California, Berkeley</span>
              <span class="author-block"><sup>2</sup> Massachusetts Institute of Technology</span>
              <span class="author-block"><sup>3</sup> Stanford University</span>
              <span class="author-block"><sup>4</sup> Tsinghua University</span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
              <br><span style="color: #dc2626; font-weight: bold;">Accepted by NeurIPS 2025 as a Spotlight Paper</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2505.18875" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/svg-project/Sparse-VideoGen/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2505.18875" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                <!-- Twitter link -->
                <span class="link-block">
                  <a href="." target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-twitter"></i>
                    </span>
                    <span>Twitter</span>
                  </a>
                </span>

                <br>

                <!-- Flashinfer Kernel -->
                <span class="link-block">
                  <a href="https://docs.flashinfer.ai/api/sparse.html" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-bolt"></i>
                    </span>
                    <span>Attention Kernel</span>
                  </a>
                </span>

                <!-- Flash k-Means Kernel -->
                <span class="link-block">
                  <a href="https://github.com/svg-project/flash-kmeans" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-project-diagram"></i>
                    </span>
                    <span>Flash k-Means Kernel</span>
                  </a>
                </span>

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Image Gallery Section -->
  <section class="section hero" style="padding: 0px 0;">
    <div class="container is-fluid">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <h1 class="title is-4 mb-4">Gallery - All videos are generated by SVG2!</h1>

          <!-- Custom Grid Layout -->
          <div class="image-gallery-grid">
            <!-- Row 1: 3 equal squares -->
            <div class="gallery-item item-1-1 clickable-image" data-video="assets/gallery/1-1.mp4">
              <video autoplay muted loop style="width: 100%; height: 100%; object-fit: cover; border-radius: 12px;">
                <source src="assets/gallery/1-1.mp4" type="video/mp4">
              </video>
            </div>
            <div class="gallery-item item-1-2 clickable-image" data-video="assets/gallery/1-2.mp4">
              <video autoplay muted loop style="width: 100%; height: 100%; object-fit: cover; border-radius: 12px;">
                <source src="assets/gallery/1-2.mp4" type="video/mp4">
              </video>
            </div>
            <div class="gallery-item item-1-3 clickable-image" data-video="assets/gallery/1-3.mp4">
              <video autoplay muted loop style="width: 100%; height: 100%; object-fit: cover; border-radius: 12px;">
                <source src="assets/gallery/1-3.mp4" type="video/mp4">
              </video>
            </div>

            <!-- Row 2: 2 rectangles -->
            <div class="gallery-item item-2-1 clickable-image" data-video="assets/gallery/2-1.mp4">
              <video autoplay muted loop style="width: 100%; height: 100%; object-fit: cover; border-radius: 12px;">
                <source src="assets/gallery/2-1.mp4" type="video/mp4">
              </video>
            </div>
            <div class="gallery-item item-2-2 clickable-image" data-video="assets/gallery/2-2.mp4">
              <video autoplay muted loop style="width: 100%; height: 100%; object-fit: cover; border-radius: 12px;">
                <source src="assets/gallery/2-2.mp4" type="video/mp4">
              </video>
            </div>
            <div class="gallery-item item-2-3 clickable-image" data-video="assets/gallery/2-3.mp4">
              <video autoplay muted loop style="width: 100%; height: 100%; object-fit: cover; border-radius: 12px;">
                <source src="assets/gallery/2-3.mp4" type="video/mp4">
              </video>
            </div>
            <div class="gallery-item item-2-4 clickable-image" data-video="assets/gallery/2-4.mp4">
              <video autoplay muted loop style="width: 100%; height: 100%; object-fit: cover; border-radius: 12px;">
                <source src="assets/gallery/2-4.mp4" type="video/mp4">
              </video>
            </div>

            <!-- Row 3: Mixed layout -->
            <div class="gallery-item item-3-1 clickable-image" data-video="assets/gallery/3-1.mp4">
              <video autoplay muted loop style="width: 100%; height: 100%; object-fit: cover; border-radius: 12px;">
                <source src="assets/gallery/3-1.mp4" type="video/mp4">
              </video>
            </div>
            <div class="gallery-item item-3-2 clickable-image" data-video="assets/gallery/3-2.mp4">
              <video autoplay muted loop style="width: 100%; height: 100%; object-fit: cover; border-radius: 12px;">
                <source src="assets/gallery/3-2.mp4" type="video/mp4">
              </video>
            </div>
            <div class="gallery-item item-3-3 clickable-image" data-video="assets/gallery/3-3.mp4">
              <video autoplay muted loop style="width: 100%; height: 100%; object-fit: cover; border-radius: 12px;">
                <source src="assets/gallery/3-3.mp4" type="video/mp4">
              </video>
            </div>
            <div class="gallery-item item-3-4 clickable-image" data-video="assets/gallery/3-4.mp4">
              <video autoplay muted loop style="width: 100%; height: 100%; object-fit: cover; border-radius: 12px;">
                <source src="assets/gallery/3-4.mp4" type="video/mp4">
              </video>
            </div>
            <div class="gallery-item item-3-5 clickable-image" data-video="assets/gallery/3-5.mp4">
              <video autoplay muted loop style="width: 100%; height: 100%; object-fit: cover; border-radius: 12px;">
                <source src="assets/gallery/3-5.mp4" type="video/mp4">
              </video>
            </div>
            <div class="gallery-item item-3-6 clickable-image" data-video="assets/gallery/3-6.mp4">
              <video autoplay muted loop style="width: 100%; height: 100%; object-fit: cover; border-radius: 12px;">
                <source src="assets/gallery/3-6.mp4" type="video/mp4">
              </video>
            </div>

            <!-- Bottom row -->
            <div class="gallery-item item-4-1 clickable-image" data-video="assets/gallery/4-1.mp4">
              <video autoplay muted loop style="width: 100%; height: 100%; object-fit: cover; border-radius: 12px;">
                <source src="assets/gallery/4-1.mp4" type="video/mp4">
              </video>
            </div>
            <div class="gallery-item item-4-2 clickable-image" data-video="assets/gallery/4-2.mp4">
              <video autoplay muted loop style="width: 100%; height: 100%; object-fit: cover; border-radius: 12px;">
                <source src="assets/gallery/4-2.mp4" type="video/mp4">
              </video>
            </div>
            <div class="gallery-item item-4-3 clickable-image" data-video="assets/gallery/4-3.mp4">
              <video autoplay muted loop style="width: 100%; height: 100%; object-fit: cover; border-radius: 12px;">
                <source src="assets/gallery/4-3.mp4" type="video/mp4">
              </video>
            </div>
            <div class="gallery-item item-4-4 clickable-image" data-video="assets/gallery/4-4.mp4">
              <video autoplay muted loop style="width: 100%; height: 100%; object-fit: cover; border-radius: 12px;">
                <source src="assets/gallery/4-4.mp4" type="video/mp4">
              </video>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-4">TL;DR</h2>
          <div class="content has-text-justified">
            <p>
              Sparse VideoGen2 is a <strong>training-free</strong> framework that accelerates video diffusion models
              inference, using an
              innovative <strong>semantic-aware permutation</strong> technique and efficient <strong>dynamic attention
                kernels</strong>. It offers pareto-frontier performance with high fidelity and high generation speed.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Teaser video-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <!-- Your video here -->
          <source src="assets/video/0919Final_LosslessQuality.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          Sparse VideoGen 2 achieves state-of-the-art video quality.
        </h2>
      </div>
    </div>
  </section>
  <!-- End teaser video -->
  <!-- Additional video section -->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="speedup" autoplay controls muted loop height="100%">
          <!-- Your video here -->
          <source src="assets/video/0919Final_FlashGeneration.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          Sparse VideoGen 2 achieves 2× speedup in video generation.
        </h2>
      </div>
    </div>
  </section>

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <!-- Logo section -->
          <div class="container has-text-centered">
            <figure class="image is-inline-block" style="width: 40%;">
              <img src="assets/Logo2_dark.png" alt="Sparse VideoGen2 Logo" class="clickable-image">
            </figure>
          </div>
          <h2 class="title is-4">Overview</h2>
          <div class="content has-text-justified">
            The Sparse VideoGen2 framework is designed to address the computational bottlenecks in state-of-the-art
            video generation models like Wan and HunyuanVideo. Without requiring any fine-tuning of
            pre-trained models, it leverages the inherent sparsity within attention maps to significantly speed up the
            inference process.

            <br><br>

            Through a co-design of algorithms and systems, Sparse VideoGen2 provides an end-to-end
            solution that achieves measurable speedups on real-world hardware, offering greater efficiency and lower
            costs for video generation tasks.


          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <!-- Challenge of 3D Full Attention in Video Generation -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-4">Limitations of Current Sparse Attention Algorithms</h2>
          <div class="content has-text-justified">
            <p>
              While current video generation models are powerful, the <strong>3D Full Attention mechanism</strong> is
              extremely slow and requires significant computation resources. While various sparse
              attention methods have been proposed to mitigate the high cost of full attention, they fall short due to
              two critical drawbacks.

              <br><br>
              First, these methods suffer from <strong>inaccurate identification</strong>. Existing sparse methods often
              rely on predefined, static patterns (e.g., local windows or strided attention) to select tokens.
              Consequently, the aggregated activations become less representative, making the selection of critical
              tokens inaccurate and degrade the video quality.

              <br><br>
              Second, these methods lead to <strong>significant computation waste</strong>. Even when these methods
              reduce the total number of calculations, the scattered critical tokens introduce irregular and scattered
              memory access patterns. Modern hardware like GPUs can <strong>not achieve peak performance</strong> when
              accessing data
              from incontiguous blocks of memory.

            <div class="container has-text-centered">
              <figure class="image is-inline-block" style="width: 100%;">
                <img src="assets/Limitation_And_Motivation.png"
                  alt="Semantic-Aware Permutation mitigates the inaccurate identification and computation waste problems"
                  class="clickable-image">
                <figcaption style="font-size: 0.8em; color: gray;">
                  Figure: Semantic-Aware Permutation mitigates the inaccurate identification and computation waste
                  problems.
                </figcaption>
              </figure>
            </div>
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Unveiling Inherent Sparsity in Attention Mechanisms -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-4">Efficient Sparse Attention via Semantic-Aware Permutation</h2>

          <div class="content has-text-justified">
            <p>
              We propose to <strong>rearrange the tokens</strong> in a way that brings semantically similar
              tokens closer together in
              global memory. We employ a lightweight, <strong>Semantic-Aware Permutation</strong> strategy to achieve
              this.

              <br><br>

              This step happens on-the-fly for each timestep and layers, and is crucial for the efficiency of the
              subsequent
              clustering and attention approximation stages. We apply different permutation for query and key / value
              tokens to further improve the accurateness of the permutation.
            </p>
          </div>

          <!-- Demo video section -->
          <section class="hero teaser">
            <div class="container is-max-desktop">
              <div class="hero-body">
                <video poster="" id="algorithm-demo" autoplay controls muted loop height="100%">
                  <!-- Your video here -->
                  <source src="assets/video/0919Final_Algorithm.mp4" type="video/mp4">
                </video>
                <h2 class="subtitle has-text-centered">
                  Semantic-Aware Permutation rearranges the tokens in a way that brings semantically similar tokens
                  closer together in memory.
                </h2>
              </div>
            </div>
          </section>


          <div class="content has-text-justified">
            <h3 class="title is-5"> Identify semantically similar tokens using k-means clustering</h3>
            We apply an <strong>efficient k-means clustering algorithm</strong> to decide the semantic similarity of the
            tokens and group them into several clusters, where each cluster represents a set of semantically
            similar tokens. The <strong>centroid of each cluster</strong> can be viewed as the <strong>representative
            </strong> of the cluster.

            <br><br>

            More clusters can group the tokens in a more
            fine-grained manner, but might introduce large overhead when executing the k-means algorithm and reduce the
            attention kernel's performance. Vice versa. We find that 100~200 clusters for query tokens, and 500~1000
            clusters for key / value tokens are a good
            trade-off. Efficiency results can be found below.


            <h3 class="title is-5">Approximate the attention map through Centroid-based Top-P estimation</h3>
            With tokens grouped into clusters, we now decide the sparse attention patterns. Since centroids represent
            the clusters, we can approximate attention weight criticality based on them. Exact attention is computed
            only between queries centroids and key / value centroids, serving as a proxy for the full attention.

            <br><br>

            We then
            use a <strong>Centroid-based Top-P Estimation</strong> to identify the most important weights. Top-P
            attention ensures quality while adapting the attention budget. Thus, we only compute exact attention for
            important weights, significantly reducing computation.

            <div class="container has-text-centered">
              <figure class="image is-inline-block" style="width: 100%;">
                <img src="assets/Algorithm_Overview.png" alt="The overall pipeline of Semantic-Aware Permutation"
                  class="clickable-image">
                <figcaption style="font-size: 0.8em; color: gray;">
                  Figure: The overall pipeline of Semantic-Aware Permutation. We use k-means clustering to identify
                  the
                  semantically similar tokens, then identify the sparse attention patterns based on the centroids.
                </figcaption>
              </figure>
            </div>

            <h3 class="title is-5"> System Optimizations for Semantic-Aware Permutation</h3>
            To further improve the performance of Semantic-Aware Permutation, we propose <strong>Centroids
              Cache</strong> to
            reduce the overhead of k-means by utilizing redundancy between timesteps. Compared with naively applying
            k-means for each timestep, Centroids Cache offers 76× speedup.

            <br><br>

            By introducing semantic-aware permutation, we successfully reached the ideal hardware efficiency on GPUs.
            Compared with the scattered attention pattern without permutation, dynamic attention kernel offers 1.7× ~
            1.8× speedup.

            <div class="container has-text-centered">
              <figure class="image is-inline-block" style="width: 100%;">
                <img src="assets/Centroid_Cache_And_Kernel.png"
                  alt="Centroids Cache and Dynamic Attention Kernel optimizations" class="clickable-image">
                <figcaption style="font-size: 0.8em; color: gray;">
                  Figure: The overall pipeline of Semantic-Aware Permutation. We use k-means clustering to identify
                  the
                  semantically similar tokens, then identify the sparse attention patterns based on the centroids.
                </figcaption>
              </figure>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Introducing Sparse VideoGen 2 (SVG2) -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-4">Efficient Dynamic Block Size Attention kernels</h2>
          <div class="content has-text-justified">
            <p>
              Standard <strong>block-sparse attention kernels</strong> are optimized for fixed-size block sizes, which
              is inefficient for our clustered approach where each cluster can have a different number of tokens.

              <br><br>

              To address this, we developed <strong>Efficient Dynamic Block Size Attention Kernels</strong>, supporting
              both FlashAttention-2 and FlashAttention-3 algorithms.
              These custom CUDA kernels are designed to handle variable-sized
              blocks of data, allowing for highly efficient computation on the sparse, clustered attention map. This
              hardware-level optimization ensures that our theoretical gains from approximation translate into
              real-world speedups.

              <br><br>

              Our kernel has a very loose dependency on the cluster size on key / value tokens, making it highly
              efficient even for small cluster size and enables a large number of clusters. For query tokens, we find
              that having a larger block size (meaning that the number of blocks is smaller) is important for high
              TFLOPs. Ablations on the number of clusters can be found below.

            <div class="container has-text-centered">
              <figure class="image is-inline-block" style="width: 100%;">
                <img src="assets/Attention_Kernel_Performance_Ablation.png"
                  alt="Efficient Dynamic Block Size Attention Kernels performance" class="clickable-image">
                <figcaption style="font-size: 0.8em; color: gray;">
                  Figure: Efficient Dynamic Block Size Attention Kernels achieves high performance.
                </figcaption>
              </figure>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Quantitative Evaluation -->
  <section class="section hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-4">Quantitative Evaluation</h2>
          <div class="content has-text-justified">
            <p>
              We evaluate the performance of Sparse VideoGen 2 on Wan 2.1 and HunyuanVideo.
            </p>
          </div>

          <!-- Insert the figure here -->
          <div class="container has-text-centered">
            <figure class="image is-inline-block" style="width: 100%;">
              <img src="assets/Result_Table.png" alt="Quantitative Evaluation Results" class="clickable-image">
            </figure>
          </div>

        </div>
      </div>
    </div>
  </section>


  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code id="bibtex-code">
@article{yang2025sparse,
  title={Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via Semantic-Aware Permutation},
  author={Yang, Shuo and Xi, Haocheng and Zhao, Yilong and Li, Muyang and Zhang, Jintao and Cai, Han and Lin, Yujun and Li, Xiuyu and Xu, Chenfeng and Peng, Kelly and others},
  journal={arXiv preprint arXiv:2505.18875},
  year={2025}
}
    </code></pre>
      <button class="button is-dark" onclick="copyBibTex()">Copy BibTeX</button>
    </div>
  </section>

  <script>
    function copyBibTex() {
      var copyText = document.getElementById("bibtex-code").innerText;
      navigator.clipboard.writeText(copyText).then(function () {
        alert("BibTeX copied to clipboard!");
      }, function (err) {
        alert("Failed to copy BibTeX: ", err);
      });
    }

    // Picture Modal Box Function
    let currentImageIndex = 0;
    let images = [];

    document.addEventListener('DOMContentLoaded', function () {
      // Get all clickable images
      images = Array.from(document.querySelectorAll('.clickable-image'));
      const modal = document.getElementById('imageModal');
      const modalImg = document.getElementById('modalImage');
      const modalCaption = document.getElementById('modalCaption');
      const closeBtn = document.querySelector('.image-modal-close');
      const prevBtn = document.querySelector('.image-modal-prev');
      const nextBtn = document.querySelector('.image-modal-next');

      // 为每个图片添加点击事件
      images.forEach((img, index) => {
        img.addEventListener('click', function () {
          currentImageIndex = index;
          showModal(img);
        });
      });

      // 显示模态框
      function showModal(img) {
        modal.style.display = 'block';

        // 检查是否是视频元素
        if (img.hasAttribute('data-video')) {
          const videoSrc = img.getAttribute('data-video');

          // 隐藏图片元素，显示视频
          modalImg.style.display = 'none';

          // 创建或获取视频元素
          let modalVideo = document.getElementById('modalVideo');
          if (!modalVideo) {
            modalVideo = document.createElement('video');
            modalVideo.id = 'modalVideo';
            modalVideo.className = 'image-modal-content';
            modalVideo.controls = true;
            modalVideo.autoplay = true;
            modalVideo.muted = true;
            modalVideo.loop = true;
            modalVideo.style.maxWidth = '90%';
            modalVideo.style.maxHeight = '90%';
            modalVideo.style.objectFit = 'contain';
            modal.appendChild(modalVideo);
          }

          modalVideo.src = videoSrc;
          modalVideo.style.display = 'block';
          modalCaption.textContent = 'Video: ' + videoSrc.split('/').pop();
        }
        // 检查是否是占位符元素
        else if (img.hasAttribute('data-placeholder')) {
          // 隐藏视频元素，显示图片
          modalImg.style.display = 'block';
          const modalVideo = document.getElementById('modalVideo');
          if (modalVideo) modalVideo.style.display = 'none';
          // 为占位符创建一个临时的canvas图片
          const canvas = document.createElement('canvas');
          canvas.width = 400;
          canvas.height = 300;
          const ctx = canvas.getContext('2d');

          // 获取元素的背景渐变色
          const computedStyle = window.getComputedStyle(img);
          const bgImage = computedStyle.backgroundImage;

          // 创建渐变背景
          let gradient;
          if (bgImage && bgImage !== 'none') {
            gradient = ctx.createLinearGradient(0, 0, canvas.width, canvas.height);
            gradient.addColorStop(0, '#667eea');
            gradient.addColorStop(1, '#764ba2');
          } else {
            gradient = '#f0f0f0';
          }

          ctx.fillStyle = gradient;
          ctx.fillRect(0, 0, canvas.width, canvas.height);

          // 添加文字
          ctx.fillStyle = 'white';
          ctx.font = 'bold 24px Arial';
          ctx.textAlign = 'center';
          ctx.textBaseline = 'middle';
          ctx.fillText(img.getAttribute('data-placeholder'), canvas.width / 2, canvas.height / 2);
          ctx.fillText('(占位符)', canvas.width / 2, canvas.height / 2 + 40);

          modalImg.src = canvas.toDataURL();
          modalImg.alt = img.getAttribute('data-placeholder');
          modalCaption.textContent = img.getAttribute('data-placeholder') || '占位符';
        } else {
          // 处理普通图片
          modalImg.style.display = 'block';
          const modalVideo = document.getElementById('modalVideo');
          if (modalVideo) modalVideo.style.display = 'none';

          modalImg.src = img.src;
          modalImg.alt = img.alt;
          modalCaption.textContent = img.alt || '图片';
        }

        document.body.style.overflow = 'hidden'; // 防止背景滚动
      }

      // 关闭模态框
      function closeModal() {
        modal.style.display = 'none';
        document.body.style.overflow = 'auto'; // 恢复滚动
      }

      // Show previous image
      function showPrevImage() {
        currentImageIndex = (currentImageIndex - 1 + images.length) % images.length;
        const img = images[currentImageIndex];
        showModal(img);
      }

      // Show next image
      function showNextImage() {
        currentImageIndex = (currentImageIndex + 1) % images.length;
        const img = images[currentImageIndex];
        showModal(img);
      }

      // Event Listener
      closeBtn.addEventListener('click', closeModal);
      prevBtn.addEventListener('click', showPrevImage);
      nextBtn.addEventListener('click', showNextImage);

      // Click modal box background to close
      modal.addEventListener('click', function (e) {
        if (e.target === modal) {
          closeModal();
        }
      });

      // Keyboard Navigation
      document.addEventListener('keydown', function (e) {
        if (modal.style.display === 'block') {
          switch (e.key) {
            case 'Escape':
              closeModal();
              break;
            case 'ArrowLeft':
              showPrevImage();
              break;
            case 'ArrowRight':
              showNextImage();
              break;
          }
        }
      });

      // Prevent modal box image click event from bubbling
      modalImg.addEventListener('click', function (e) {
        e.stopPropagation();
      });

      // Touch slide support (mobile devices)
      let startX = 0;
      let startY = 0;

      modalImg.addEventListener('touchstart', function (e) {
        startX = e.touches[0].clientX;
        startY = e.touches[0].clientY;
      });

      modalImg.addEventListener('touchend', function (e) {
        if (!startX || !startY) return;

        let endX = e.changedTouches[0].clientX;
        let endY = e.changedTouches[0].clientY;

        let diffX = startX - endX;
        let diffY = startY - endY;

        // Only trigger switch when horizontal slide distance is greater than vertical slide distance
        if (Math.abs(diffX) > Math.abs(diffY) && Math.abs(diffX) > 50) {
          if (diffX > 0) {
            showNextImage(); // Slide left, show next image
          } else {
            showPrevImage(); // Slide right, show previous image
          }
        }

        startX = 0;
        startY = 0;
      });
    });
  </script>
  <!--End BibTex citation -->


  <!-- Picture Modal Box -->
  <div id="imageModal" class="image-modal">
    <span class="image-modal-close">&times;</span>
    <span class="image-modal-nav image-modal-prev">&#10094;</span>
    <span class="image-modal-nav image-modal-next">&#10095;</span>
    <img class="image-modal-content" id="modalImage" alt="Enlarge image">
    <div class="image-modal-caption" id="modalCaption"></div>
  </div>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the
              footer. <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>